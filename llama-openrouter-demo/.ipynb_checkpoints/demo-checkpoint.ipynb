{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama-Index with OpenRouter Function Calling Demo\n",
    "\n",
    "This notebook demonstrates how to use llama-index with OpenRouter to create a function calling agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.llms.openrouter import OpenRouter\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key from environment\n",
    "api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENROUTER_API_KEY environment variable is not set\")\n",
    "\n",
    "# Set up a local embedding model (in case we want to use vector embeddings later)\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions\n",
    "\n",
    "Let's define some example functions that our agent can call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_weather(location: str) -> str:\n",
    "    \"\"\"\n",
    "    Search for the current weather in a given location.\n",
    "    \n",
    "    Args:\n",
    "        location: The city or location to get weather for\n",
    "        \n",
    "    Returns:\n",
    "        A string describing the weather\n",
    "    \"\"\"\n",
    "    # This is a mock function - in a real application, you would call a weather API\n",
    "    return f\"The weather in {location} is currently sunny with a temperature of 72Â°F.\"\n",
    "\n",
    "def calculate_mortgage(principal: float, interest_rate: float, years: int) -> str:\n",
    "    \"\"\"\n",
    "    Calculate monthly mortgage payment.\n",
    "    \n",
    "    Args:\n",
    "        principal: The loan amount in dollars\n",
    "        interest_rate: Annual interest rate (as a percentage, e.g., 5.5 for 5.5%)\n",
    "        years: Loan term in years\n",
    "        \n",
    "    Returns:\n",
    "        A string with the monthly payment amount\n",
    "    \"\"\"\n",
    "    monthly_rate = interest_rate / 100 / 12\n",
    "    num_payments = years * 12\n",
    "    monthly_payment = principal * (monthly_rate * (1 + monthly_rate) ** num_payments) / ((1 + monthly_rate) ** num_payments - 1)\n",
    "    return f\"For a ${principal:,.2f} loan with {interest_rate}% interest over {years} years, the monthly payment would be ${monthly_payment:,.2f}\"\n",
    "\n",
    "# Create function tools from our functions\n",
    "weather_tool = FunctionTool.from_defaults(fn=search_weather)\n",
    "mortgage_tool = FunctionTool.from_defaults(fn=calculate_mortgage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize OpenRouter LLM\n",
    "\n",
    "Now let's initialize the OpenRouter LLM and set it as the default for llama-index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenRouter LLM\n",
    "# Using a model that supports function calling\n",
    "llm = OpenRouter(\n",
    "    api_key=api_key,\n",
    "    model=\"anthropic/claude-3-opus-20240229\",  # You can change to another model that supports function calling\n",
    ")\n",
    "\n",
    "# Set the LLM and embedding model as the defaults for llama-index\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a ReAct Agent\n",
    "\n",
    "Let's create a ReAct agent with our tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ReAct agent with our tools\n",
    "agent = ReActAgent.from_tools(\n",
    "    [weather_tool, mortgage_tool],\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Agent\n",
    "\n",
    "Now let's test our agent with some example queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test weather query\n",
    "response = agent.chat(\"What's the weather in San Francisco?\")\n",
    "print(f\"Agent response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test mortgage calculation query\n",
    "response = agent.chat(\"Calculate mortgage payment for a $500,000 loan at 4.5% for 30 years\")\n",
    "print(f\"Agent response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Your Own Queries\n",
    "\n",
    "Now you can try your own queries to see how the agent responds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own query\n",
    "your_query = \"What's the weather in Tokyo?\"\n",
    "response = agent.chat(your_query)\n",
    "print(f\"Query: {your_query}\")\n",
    "print(f\"Agent response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Retrieval Example\n",
    "\n",
    "Now let's demonstrate how to combine function calling with document retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, Document\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "# Create some sample documents for our knowledge base\n",
    "documents = [\n",
    "    Document(text=\"Python is a high-level, interpreted programming language known for its readability and versatility. It was created by Guido van Rossum and first released in 1991.\"),\n",
    "    Document(text=\"JavaScript is a programming language that is one of the core technologies of the World Wide Web. It was created by Brendan Eich in 1995 while he was working at Netscape Communications Corporation.\"),\n",
    "    Document(text=\"Rust is a multi-paradigm, general-purpose programming language designed for performance and safety, especially safe concurrency. It was created at Mozilla Research by Graydon Hoare in 2010.\"),\n",
    "    Document(text=\"TypeScript is a programming language developed and maintained by Microsoft. It is a strict syntactical superset of JavaScript and adds optional static typing to the language. It was designed by Anders Hejlsberg in 2012.\"),\n",
    "]\n",
    "\n",
    "# Create a vector index from our documents\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# Create a query engine from the index\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# Create a query engine tool\n",
    "query_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=query_engine,\n",
    "    name=\"programming_language_kb\",\n",
    "    description=\"Useful for answering questions about programming languages like Python, JavaScript, Rust, and TypeScript.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new agent with both function tools and the query engine tool\n",
    "combined_agent = ReActAgent.from_tools(\n",
    "    [weather_tool, mortgage_tool, query_tool],\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a query about programming languages\n",
    "programming_query = \"When was Python created and who created it?\"\n",
    "response = combined_agent.chat(programming_query)\n",
    "print(f\"Query: {programming_query}\")\n",
    "print(f\"Agent response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a mixed query that requires both knowledge retrieval and function calling\n",
    "mixed_query = \"What's the weather in New York and when was JavaScript created?\"\n",
    "response = combined_agent.chat(mixed_query)\n",
    "print(f\"Query: {mixed_query}\")\n",
    "print(f\"Agent response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated how to use llama-index with OpenRouter to create a function calling agent and combine it with document retrieval. You can extend this example by adding more functions, using different models, or integrating with other llama-index features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}